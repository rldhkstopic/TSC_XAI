{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing from LPI radar signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "raw_data_path = \"/data/kiwan/dataset-CWD-1000test/\"\n",
    "signal_types = ['Barker', 'Costas', 'Frank', 'LFM', 'P1', 'P2', 'P3', 'P4', 'T1', 'T2', 'T3', 'T4']\n",
    "\n",
    "output_json = 'LPI12 dataset test.json'\n",
    "\n",
    "signals_dict = {}\n",
    "\n",
    "def extract_snr_from_filename(filename):\n",
    "    match = re.search(r'snr-?0*(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "for signal in signal_types:\n",
    "    signal_csvs = glob.glob(os.path.join(raw_data_path, signal, '*.csv'))\n",
    "    signals_dict[signal] = {}  \n",
    "\n",
    "    for csv_file in signal_csvs:\n",
    "        snr_value = extract_snr_from_filename(csv_file)\n",
    "        print(f\"Processing {csv_file} with SNR {snr_value}\", end='\\r')\n",
    "    \n",
    "        if snr_value is not None:\n",
    "            df = pd.read_csv(csv_file, header=None)\n",
    "        \n",
    "            if snr_value not in signals_dict[signal]:\n",
    "                signals_dict[signal][snr_value] = []\n",
    "        \n",
    "            signals_dict[signal][snr_value].extend(df.values.tolist())\n",
    "        else:\n",
    "            print(f\"Warning: SNR not found in {csv_file}\")\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(signals_dict, f, indent=4)\n",
    "\n",
    "print(f\"All signals have been merged by SNR into {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models._config as c\n",
    "from models.LSTM import BiLSTM\n",
    "from models.Attention import Transformer\n",
    "from explainer import LRP\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training parameters')\n",
    "parser.add_argument('-m', '--mode', type=str, default='train', help='Mode of operation (train/eval)')\n",
    "\n",
    "parser.add_argument('-smin','--snr_min', type=int, default=0, help='Minimum SNR value')\n",
    "parser.add_argument('-smax','--snr_max', type=int, default=16, help='Maximum SNR value')\n",
    "parser.add_argument('--split_size', type=float, default=0.8, help='Train/Test split size')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "parser.add_argument('--num_epochs', type=int, default=500, help='Number of epochs for training')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for optimizer')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay for optimizer')\n",
    "parser.add_argument('--input_size', type=int, default=2, help='Input size for the model')\n",
    "parser.add_argument('--hidden_size', type=int, default=128, help='Hidden size for the model')\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Number of layers in the model')\n",
    "parser.add_argument('--num_classes', type=int, default=len(c.signalTypes[:c.typeSize]), help='Number of output classes')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "params = {\n",
    "    'snr_min': args.snr_min,\n",
    "    'snr_max': args.snr_max,\n",
    "    'split_size': args.split_size,\n",
    "    'batch_size': args.batch_size,\n",
    "    'num_epochs': args.num_epochs,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'weight_decay': args.weight_decay,\n",
    "    'input_size': args.input_size,\n",
    "    'hidden_size': args.hidden_size,\n",
    "    'num_layers': args.num_layers,\n",
    "    'num_classes': args.num_classes\n",
    "}\n",
    "\n",
    "def train_eachset(dataset, mtype='LSTM'):\n",
    "    print(f\"Size of train dataset: {len(dataset)}\")\n",
    "    for snr in range(params['snr_min'], params['snr_max']+1, 2):\n",
    "        print(f\"\\nTraining model for SNR: {snr}...\")\n",
    "        ckpt = os.path.join(\"./ckpts/\", snr_str:=f\"SNR-{snr}dB\" if snr != 0 else \" 0dB\")\n",
    "        os.makedirs(ckpt, exist_ok=True)\n",
    "        \n",
    "        model = BiLSTM(params['input_size'], params['hidden_size'], params['num_layers'], params['num_classes'])\n",
    "        model.to(c.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # criterion = LabelSmoothingLoss(classes=params['num_classes'], smoothing=0.1).to(c.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], \n",
    "                                                    weight_decay=params['weight_decay'])\n",
    "        \n",
    "        snr_dataset = [(data, label, data_snr, length) for data, label, data_snr, length in dataset if data_snr == snr]\n",
    "        snr_loader = DataLoader(snr_dataset, batch_size=params['batch_size'], shuffle=True, collate_fn=model.collate)\n",
    "            \n",
    "        best_state, best_loss = model.train_model(snr_loader, criterion, optimizer, params['num_epochs'], c.device, snr_str, ckpt)\n",
    "\n",
    "        save_point = f'{ckpt}/{mtype}_{snr_str}_{best_loss:.4f}.pt'\n",
    "        \n",
    "        torch.save(best_state, save_point)\n",
    "        print(f\"Model checkpoint saved at {save_point}\")\n",
    "\n",
    "def train_set(dataset):\n",
    "    print(f\"Size of train dataset: {len(dataset)}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = BiLSTM(params['input_size'], params['hidden_size'], params['num_layers'], params['num_classes'])\n",
    "    \n",
    "    if torch.cuda.device_count()>1:\n",
    "        print(\"use device parallelly : \", torch.cuda.device_count(),\"devices\")\n",
    "        model = nn.DataParallel(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "    model.to(c.device)\n",
    "    \n",
    "    \n",
    "    snr_loader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True, collate_fn=model.collate)\n",
    "    best_state, best_loss = model.module.train_model(snr_loader, criterion, optimizer, params['num_epochs'], c.device)\n",
    "\n",
    "\n",
    "    torch.save(best_state, f'./ckpts/result_loss_{best_loss:.4f}.pt')\n",
    "    print(\"Train is done.\")\n",
    "        \n",
    "def eval_set(dataset, mtype='LSTM'):\n",
    "    print(f\"Size of test dataset: {len(dataset)}\")\n",
    "    for snr in range(params['snr_min'], params['snr_max']+1, 2):\n",
    "        ckpt = os.path.join(\"./ckpts/\", snr_str:=f\"SNR-{snr}dB\" if snr != 0 else \" 0dB\")\n",
    "        ckpts = [f for f in os.listdir(ckpt) if f.endswith(\".pt\") and f.startswith(\"LSTM\")]\n",
    "        \n",
    "        model = BiLSTM(params['input_size'], params['hidden_size'], params['num_layers'], params['num_classes']).to(c.device)\n",
    "        model.load_state_dict(torch.load(f\"{ckpt}/{ckpts[0]}\"))\n",
    "        model.eval()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        snr_dataset = [(data, label, data_snr, length) for data, label, data_snr, length in dataset if data_snr == snr]\n",
    "        snr_loader = DataLoader(snr_dataset, batch_size=params['batch_size'], shuffle=True, collate_fn=model.collate)\n",
    "        \n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in snr_loader:\n",
    "                data, labels, length = batch[0].to(c.device), batch[1].to(c.device), batch[3]\n",
    "                outputs = model(data, length)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        acc = correct / total * 100\n",
    "        avg_loss = total_loss / len(snr_loader)\n",
    "        \n",
    "        print(f\"SNR -{snr}dB | Accuracy: {acc:.2f}% | Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def explain_set(dataset, mtype='LSTM'):\n",
    "    \"\"\"\n",
    "    설명 및 기여도 분석을 위한 함수.\n",
    "    주어진 데이터셋과 모델을 사용하여 각 데이터에 대한 LRP 기여도 계산 및 시각화.\n",
    "    \n",
    "    Parameters:\n",
    "        - dataset: 입력 데이터셋\n",
    "        - mtype: 모델 타입 ('LSTM', 'Attention' 등)\n",
    "    \"\"\"\n",
    "    print(f\"Size of explain dataset: {len(dataset)}\")\n",
    "    for snr in range(params['snr_min'], params['snr_max']+1, 2):\n",
    "        print(f\"\\nExplaining model for SNR: {snr}...\")\n",
    "        ckpt = os.path.join(\"./ckpts/\", snr_str := f\"-{snr}dB\" if snr != 0 else \"0dB\")\n",
    "        ckpts = [f for f in os.listdir(ckpt) if f.endswith(\".pt\")  and f.startswith(\"LSTM\")]\n",
    "\n",
    "        # 모델 타입에 따라 BiLSTM 또는 Transformer 로드\n",
    "        if mtype == 'LSTM':\n",
    "            model = BiLSTM(params['input_size'], params['hidden_size'],\n",
    "                           params['num_layers'], params['num_classes']).to(c.device)\n",
    "        elif mtype == 'Attention':\n",
    "            model = Transformer(params['input_size'], params['hidden_size'],\n",
    "                                params['num_layers'], params['num_classes'], nhead=8, dropout=0.3).to(c.device)\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"{ckpt}/{ckpts[0]}\"))\n",
    "        model.eval()\n",
    "        \n",
    "        # LRP 적용 객체\n",
    "        lrp = LRP(model)\n",
    "\n",
    "        # SNR에 따른 데이터 필터링\n",
    "        snr_dataset = [(data, label, data_snr) for data, label, data_snr in dataset if data_snr == snr]\n",
    "        snr_loader = DataLoader(snr_dataset, batch_size=params['batch_size'], shuffle=False, collate_fn=model.collate)\n",
    "\n",
    "        # 결과 저장 디렉토리 생성\n",
    "        results_dir = os.path.join(\"./explanations/\", f\"{mtype}_{snr_str}\")\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "        explanation_results = []\n",
    "\n",
    "        # LRP를 사용하여 기여도 분석 및 시각화\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(snr_loader):\n",
    "                data, labels = batch[0].to(c.device), batch[1].to(c.device)\n",
    "                \n",
    "                # LRP 기법으로 relevance 계산\n",
    "                relevance, _, attn_weights = lrp.get_relevance(data)\n",
    "\n",
    "                for j in range(data.size(0)):\n",
    "                    sample_data = data[j].cpu().numpy()\n",
    "                    sample_relevance = relevance[j].cpu().numpy()\n",
    "                    label = labels[j].item()\n",
    "\n",
    "                    explanation_results.append((sample_relevance, label))\n",
    "\n",
    "                    # 히트맵 시각화\n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                    time_steps = np.arange(sample_data.shape[0])\n",
    "\n",
    "                    # 원본 신호 플롯\n",
    "                    ax.plot(time_steps, sample_data[:, 0], label='Signal', color='blue')\n",
    "                    ax.set_title(f'Sample {i * params[\"batch_size\"] + j} - SNR: {snr}dB, Label: {label}')\n",
    "                    ax.set_xlabel('Time Step')\n",
    "                    ax.set_ylabel('Amplitude')\n",
    "\n",
    "                    # 히트맵 오버레이 (relevance score)\n",
    "                    im = ax.imshow(sample_relevance.T, aspect='auto', cmap='hot', alpha=0.6,\n",
    "                                   extent=[0, sample_data.shape[0],\n",
    "                                           np.min(sample_data), np.max(sample_data)])\n",
    "\n",
    "                    # Attention 가중치 시각화 추가\n",
    "                    attn_map = attn_weights[j].cpu().numpy()\n",
    "                    ax.plot(time_steps, attn_map, color='red', alpha=0.5, label='Attention Score')\n",
    "\n",
    "                    # 컬러바 추가\n",
    "                    cbar = plt.colorbar(im, ax=ax)\n",
    "                    cbar.set_label('Relevance Score')\n",
    "\n",
    "                    # 시각화 저장\n",
    "                    plot_path = os.path.join(results_dir, f\"sample_{i * params['batch_size'] + j}_heatmap.png\")\n",
    "                    plt.savefig(plot_path)\n",
    "                    plt.close()\n",
    "                    print(f\"Heatmap for sample {i * params['batch_size'] + j} saved to {plot_path}\")\n",
    "\n",
    "        print(f\"LRP explanations and visualizations for SNR {snr} saved in {results_dir}\")\n",
    "\n",
    "    return explanation_results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LRP:\n",
    "    def __init__(self, model, epsilon=1e-5):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_relevance(self, x, target=None):\n",
    "        \"\"\"\n",
    "        입력 데이터 x에 대한 relevance 점수를 계산합니다.\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 데이터 (batch_size, seq_len, input_size)\n",
    "            target (torch.Tensor): 타겟 클래스 (optional)\n",
    "        Returns:\n",
    "            relevance (torch.Tensor): relevance 점수 (batch_size, seq_len)\n",
    "            attention_weights (torch.Tensor): attention 가중치 (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # 순전파 수행\n",
    "        x.requires_grad = True\n",
    "        output, hid_outputs, attn_weights = self.forward(x, lstm_outputs=True)  # LSTM outputs: [batch_size, seq_len, hidden_size * 2]\n",
    "\n",
    "        # 예측된 클래스 선택\n",
    "        if target is None:\n",
    "            target = torch.argmax(output, dim=1)\n",
    "        else:\n",
    "            target = torch.tensor(target).to(x.device)\n",
    "\n",
    "        # 모델 예측에 대한 기여도 초기화\n",
    "        relevance = torch.zeros_like(output)\n",
    "        for i in range(output.size(0)):\n",
    "            relevance[i, target[i]] = output[i, target[i]]\n",
    "\n",
    "        # LRP relevance 역전파 계산\n",
    "        relevance = self.compute_relevance(hid_outputs, relevance)\n",
    "\n",
    "        return relevance, output, attn_weights\n",
    "\n",
    "    def forward(self, x, lstm_outputs=False):\n",
    "        \"\"\"모델의 순전파를 수행하고 LSTM hidden outputs 및 attention weights를 반환합니다.\"\"\"\n",
    "        return self.model(x, lstm_outputs)\n",
    "\n",
    "    def compute_relevance(self, hiddens, relevance):\n",
    "        \"\"\"\n",
    "        LSTM 모델에서 relevance 역전파 수행.\n",
    "        Args:\n",
    "            hiddens (torch.Tensor): LSTM hidden states (batch_size, seq_len, hidden_size * 2)\n",
    "            relevance (torch.Tensor): 모델의 출력 레벨에서 계산된 relevance\n",
    "        Returns:\n",
    "            total_relevance (torch.Tensor): 각 타임 스텝에 대한 relevance (batch_size, seq_len, hidden_size * 2)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size2 = hiddens.size()  # hidden_size2 = hidden_size * 2\n",
    "        hidden_size = hidden_size2 // 2\n",
    "        \n",
    "        # Forward LSTM에 대한 relevance 계산\n",
    "        h_fw, h_bw = hiddens[:, :, :hidden_size], hiddens[:, :, hidden_size:]\n",
    "        rel_fw, rel_bw = torch.zeros_like(h_fw), torch.zeros_like(h_bw)\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            h_t_fw = h_fw[:, t, :]  # [batch_size, hidden_size]\n",
    "            h_t_bw = h_bw[:, t, :]  # [batch_size, hidden_size]\n",
    "            \n",
    "            # Forward 방향 relevance 계산\n",
    "            z_t_fw = F.linear(h_t_fw, self.model.fc.weight[:, :hidden_size], self.model.fc.bias) + self.epsilon  # [batch_size, 4]\n",
    "            s_t_fw = relevance / (z_t_fw + self.epsilon)  # [batch_size, 4]\n",
    "            c_t_fw = torch.matmul(s_t_fw, self.model.fc.weight[:, :hidden_size])  # [batch_size, hidden_size]\n",
    "            rel_fw[:, t, :] = h_t_fw * c_t_fw  # [batch_size, hidden_size]\n",
    "\n",
    "            # Backward 방향 relevance 계산\n",
    "            z_t_bw = F.linear(h_t_bw, self.model.fc.weight[:, hidden_size:], self.model.fc.bias) + self.epsilon  # [batch_size, 4]\n",
    "            s_t_bw = relevance / (z_t_bw + self.epsilon)  # [batch_size, 4]\n",
    "            c_t_bw = torch.matmul(s_t_bw, self.model.fc.weight[:, hidden_size:])  # [batch_size, hidden_size]\n",
    "            rel_bw[:, t, :] = h_t_bw * c_t_bw  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Forward와 Backward relevance 합산\n",
    "        total_relevance = rel_fw + rel_bw  # [batch_size, seq_len, hidden_size * 2]\n",
    "        \n",
    "        return total_relevance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.RadarDataset import RadarSignalDataset  \n",
    "\n",
    "mode = 'train'\n",
    "model_type = 'LSTM'\n",
    "if __name__ == \"__main__\":\n",
    "    if mode == 'train':\n",
    "        train_dataset = RadarSignalDataset(c.TrainData, c.signalTypes[0:c.typeSize], snr_max=17)\n",
    "        # train_set(train_dataset)\n",
    "    elif mode == 'eval':\n",
    "        test_dataset = RadarSignalDataset(c.TestData, c.signalTypes[0:c.typeSize], snr_max=17)\n",
    "        eval_set(test_dataset, mtype=model_type)\n",
    "    elif mode == 'explain':\n",
    "        explain_dataset = RadarSignalDataset(c.TestData, c.signalTypes[0:c.typeSize], snr_max=17)\n",
    "        # explain_set(explain_dataset)\n",
    "    else:\n",
    "        print(\"Invalid mode of operation. Choose from 'train', 'eval', 'explain'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visdom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import models._config as c\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "    \n",
    "# Temporal Attention Layer (Zero-padding에 대한 가중치 조정 포함)\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # BiLSTM이므로 hidden_size * 2\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size)) \n",
    "        \n",
    "    def forward(self, hidden_states, mask=None):\n",
    "        \"\"\"\n",
    "        hidden_states: [batch_size, seq_len, hidden_size * 2]\n",
    "        mask: [batch_size, seq_len] - zero-padding mask\n",
    "        \"\"\"\n",
    "        attn_weights = torch.tanh(self.attn(hidden_states))  # [batch_size, seq_len, hidden_size]\n",
    "        attn_weights = attn_weights.matmul(self.v)           # [batch_size, seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.to(attn_weights.device)\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)  # Zero-padding에 대한 large negative\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # [batch_size, seq_len]에서 softmax로 중요도 결정\n",
    "        \n",
    "        # 가중치를 반영하여 각 타임 스텝의 hidden state를 곱해줌\n",
    "        context = torch.sum(hidden_states * attn_weights.unsqueeze(-1), dim=1)  # [batch_size, hidden_size * 2]\n",
    "        return context, attn_weights\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.attention = TemporalAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Bidirectional이므로 hidden_size * 2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, lengths, lstm_outputs=False):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Initial hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # PackedSequence로 변환하여 RNN/LSTM에서 패딩 무시\n",
    "        packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hn, cn) = self.lstm(packed_x, (h0, c0))  # LSTM 통과\n",
    "        \n",
    "        # 다시 패딩된 시퀀스로 변환\n",
    "        out, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        # Zero-padding Mask 생성\n",
    "        mask = torch.arange(seq_len).expand(batch_size, seq_len) < lengths.unsqueeze(1)\n",
    "        \n",
    "        # Attention with Zero-padding Mask 적용\n",
    "        context, attn_weights = self.attention(out, mask)  # Self-Attention 통과\n",
    "        \n",
    "        out_last = self.dropout(context)  # Dropout\n",
    "        out_fc = self.fc(out_last)  # Fully connected layer\n",
    "                \n",
    "        if lstm_outputs:\n",
    "            return out_fc, out, attn_weights\n",
    "        else:\n",
    "            return out_fc\n",
    "\n",
    "\n",
    "    def train_model(self, train_loader, criterion, optimizer, num_epochs, device):\n",
    "        vis = visdom.Visdom()\n",
    "        assert vis.check_connection(), \"Visdom 서버를 실행 필수 : python -m visdom.server\"\n",
    "\n",
    "        losses = []  \n",
    "        vis_window = vis.line(\n",
    "            X=torch.zeros((1,)).cpu(),\n",
    "            Y=torch.zeros((1,)).cpu(),\n",
    "            opts=dict(xlabel='Epoch', ylabel='Loss', title=f'Training Loss', legend=['Loss'])\n",
    "        )\n",
    "        \n",
    "        best_loss = float('inf')  # Best loss 초기화\n",
    "        best_state = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()  \n",
    "            running_loss = 0.0  \n",
    "            \n",
    "            progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "            \n",
    "            for batch_idx, (data_batch, labels_batch, _, lengths_batch) in progress_bar:\n",
    "                data_batch = data_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                lengths_batch = lengths_batch.cpu()  # 시퀀스 길이를 CPU로 이동\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self(data_batch, lengths_batch)  # self()는 forward()를 호출함\n",
    "                \n",
    "                # Loss 계산\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # 현재 배치 번호와 평균 손실을 tqdm에 표시\n",
    "                progress_bar.set_postfix({\n",
    "                    'Batch': f\"{batch_idx + 1}/{len(train_loader)}\",\n",
    "                    'Loss': f\"{loss.item():.4f}\"\n",
    "                })\n",
    "\n",
    "                loss.detach()\n",
    "\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            vis.line(\n",
    "                X=torch.tensor([epoch + 1]).cpu(),\n",
    "                Y=torch.tensor([avg_loss]).cpu(),\n",
    "                win=vis_window,\n",
    "                update='append'\n",
    "            )\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            # Best loss 갱신\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_state = self.state_dict()\n",
    "\n",
    "            if best_loss < 0.01:\n",
    "                break\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return best_state, best_loss\n",
    "\n",
    "    @staticmethod        \n",
    "    def collate(batch):\n",
    "        data, labels, snrs, lengths = zip(*batch)\n",
    "        data_pad = rnn_utils.pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in data], batch_first=True)\n",
    "        \n",
    "        labels = torch.tensor([c.label_mapping[label] for label in labels], dtype=torch.long)\n",
    "        snrs = torch.tensor(snrs, dtype=torch.float32)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.long)  # 시퀀스 길이를 함께 전달\n",
    "\n",
    "        return data_pad, labels, snrs, lengths\n",
    "\n",
    "\n",
    "# def train_set(dataset):\n",
    "\n",
    "print(f\"Size of train dataset: {len(train_dataset)}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = BiLSTM(params['input_size'], params['hidden_size'], params['num_layers'], params['num_classes'])\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs with DataParallel.\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to('cuda')\n",
    "else:\n",
    "    model.to(c.device)\n",
    "    \n",
    "optimizer = optim.Adam(model.module.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "snr_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=model.module.collate, num_workers=8)\n",
    "# best_state, best_loss = model.module.train_model(snr_loader, criterion, optimizer, params['num_epochs'], c.device)\n",
    "\n",
    "\n",
    "# torch.save(best_state, f'./ckpts/result_loss_{best_loss:.4f}.pt')\n",
    "# print(\"Train is done.\")\n",
    "    \n",
    "# train_set(train_dataset) # 8374MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_state, best_loss = model.module.train_model(snr_loader, criterion, optimizer, params['num_epochs'], c.device)\n",
    "\n",
    "\n",
    "torch.save(best_state, f'./ckpts/result_loss_{best_loss:.4f}.pt')\n",
    "print(\"Train is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#for j in range(data.size(0)):\n",
    "#   relevance = lrp.get_relevance(data[j]) # [seq_len, input_size]\n",
    "\n",
    "class LRP:\n",
    "    def __init__(self, model, epsilon=1e-5):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, x, lstm_outputs):\n",
    "        return self.model(x, lstm_outputs)\n",
    "    \n",
    "    def get_relevance(self, x, target=None):        \n",
    "        x.requires_grad = True\n",
    "        output, hiddens = self.forward(x, lstm_outputs=True) # [32, 4] | [32, 1495, 256]\n",
    "        # output은 모델의 예측 결과, hid_outputs는 LSTM의 각 시간 스텝의 hidden state를 포함\n",
    "        # output : [batch_size, num_classes], hiddens : [batch_size, seq_len, hidden_size]\n",
    "        target = torch.argmax(output, dim=1) if target is None else torch.tensor(target).to(x.device)\n",
    "\n",
    "        # 정답 클래스에 대한 기여도 초기화\n",
    "        relevance = torch.zeros_like(output)\n",
    "        for i in range(output.size(0)):\n",
    "            relevance[i, target[i]] = output[i, target[i]]\n",
    "        \n",
    "        relevance = relevance.unsqueeze(1).expand(-1, hiddens.size(1), -1) # [batch_size, seq_len, num_classes]\n",
    "        \n",
    "        \n",
    "        for module in reversed(list(self.model.modules())):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                relevance = self.linear_lrp(module, hiddens, relevance)\n",
    "            elif isinstance(module, nn.LSTM):\n",
    "                relevance = self.bilstm_lrp(module, hiddens, relevance)\n",
    "            elif isinstance(module, nn.ReLU) or isinstance(module, nn.Tanh):\n",
    "                relevance = self.activation_lrp(module, hiddens, relevance)\n",
    "                \n",
    "        return relevance\n",
    "\n",
    "\n",
    "    # hiddens : torch.Size([32, 1495, 256]) : [batch_size, seq_len, hidden_size] | relevance : torch.Size([32, 4])\n",
    "    def linear_lrp(self, layer, hiddens, relevance): \n",
    "        print(hiddens.size())\n",
    "        batch_size, seq_len, _ = hiddens.size()\n",
    "        \n",
    "        weight = layer.weight  # [num_classes, hidden_size]\n",
    "        bias = layer.bias      # [num_classes]\n",
    "        total_relevance = torch.zeros_like(hiddens)  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            hiddens_i = hiddens[i]                           # [seq_len, hidden_size]\n",
    "            relevance_i = relevance[i]                       # [num_classes]\n",
    "            total_relevance_i = torch.zeros_like(hiddens_i)  # [seq_len, hidden_size]\n",
    "\n",
    "            for t in range(seq_len):\n",
    "                h_t = hiddens_i[t, :]                              \n",
    "                z_t = F.linear(h_t, weight, bias) + self.epsilon   \n",
    "                s_t = relevance_i / z_t                            \n",
    "                c_t = torch.matmul(s_t, weight)                    \n",
    "\n",
    "                total_relevance_i[t, :] = h_t * c_t  # [hidden_size]\n",
    "\n",
    "            total_relevance[i] = total_relevance_i\n",
    "\n",
    "        print(total_relevance.size())  # total_relevance: [batch_size, seq_len, hidden_size]\n",
    "        return total_relevance\n",
    "    \n",
    "    def bilstm_lrp(self, layer, hiddens, relevance):\n",
    "        batch_size, seq_len, hidden_size2 = hiddens.size()\n",
    "        hidden_size = hidden_size2 // 2                     # only one-direction hidden size\n",
    "        \n",
    "        h_fw, h_bw = hiddens[:, :, :hidden_size], hiddens[:, :, hidden_size:]\n",
    "        rel_fw, rel_bw = torch.zeros_like(h_fw), torch.zeros_like(h_bw)\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            h_t_fw = h_fw[:, t, :]  # [batch_size, hidden_size]\n",
    " \n",
    "            z_t_fw = F.linear(h_t_fw, layer.weight_ih_l0, layer.bias_ih_l0) + self.epsilon\n",
    "                    \n",
    "            s_t_fw = relevance / (z_t_fw + self.epsilon)\n",
    "            c_t_fw = s_t_fw @ layer.weight_ih_l0[:hidden_size, :].t() \n",
    "            rel_fw[:, t, :] = h_t_fw * c_t_fw\n",
    "            \n",
    "            h_t_bw = h_bw[:, t, :]  # [batch_size, hidden_size]\n",
    "            z_t_bw = F.linear(h_t_bw, layer.weight_ih_l0_reverse, layer.bias_ih_l0_reverse) + self.epsilon\n",
    "            s_t_bw = relevance / (z_t_bw + self.epsilon)\n",
    "            c_t_bw = s_t_bw @ layer.weight_ih_l0[hidden_size:, :].t()\n",
    "            rel_bw[:, t, :] = h_t_bw * c_t_bw\n",
    "            \n",
    "        total_relevance = rel_fw + rel_bw\n",
    "        print(f\"total relevance: {total_relevance.size()}\")\n",
    "        return total_relevance\n",
    " \n",
    "    def compute_lstm_relevance(self, layer, x, relevance, direction='forward'):\n",
    "        \"\"\"\n",
    "        LSTM의 각 시간 스텝에 대해 relevance 계산.\n",
    "        - layer: nn.LSTM 레이어\n",
    "        - x: 입력 텐서\n",
    "        - relevance: 기여도 텐서\n",
    "        - direction: 'forward' or 'backward' (방향 선택)\n",
    "        \"\"\"\n",
    "        # 정방향 또는 역방향 LSTM의 각 시간 스텝에 대해 relevance를 계산\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 방향에 따라 시퀀스를 정방향 또는 역방향으로 순회하며 기여도 계산\n",
    "        if direction == 'forward':\n",
    "            time_steps = range(seq_len)\n",
    "        elif direction == 'backward':\n",
    "            time_steps = reversed(range(seq_len))\n",
    "        \n",
    "        # 시퀀스를 순회하면서 각 시간 스텝에 대한 기여도 계산\n",
    "        for t in time_steps:\n",
    "            h_t = layer(x[:, t, :])[0]  # 현재 시간 스텝의 LSTM 출력\n",
    "            z = h_t + self.epsilon  # 작은 epsilon 추가\n",
    "            s = relevance[:, t, :] / z  # relevance 계산\n",
    "            relevance[:, t, :] = x[:, t, :] * s\n",
    "        \n",
    "        return relevance\n",
    "    \n",
    "    def activation_lrp(self, layer, x, relevance):\n",
    "        \"\"\"\n",
    "        활성화 함수의 LRP 계산 (ReLU, Tanh 등).\n",
    "        활성화 함수에서는 LRP를 사용하여 기여도 역전파를 수행.\n",
    "        \"\"\"\n",
    "        # 활성화 함수의 입력 x에 대해 기여도를 직접 역전파\n",
    "        return relevance * (x > 0).float()  # 활성화된 뉴런만 기여도를 전달\n",
    "\n",
    "\n",
    "explain_set(explain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from explainer import LRP\n",
    "\n",
    "def explain_mode(dataset):\n",
    "    print(f\"Size of test dataset: {len(dataset)}\")\n",
    "    for snr in range(params['snr_min'], params['snr_max']+1, 2):\n",
    "        ckpt = os.path.join(\"./ckpts/\", snr_str:=f\"-{snr}dB\" if snr != 0 else \"0dB\")\n",
    "        ckpts = [f for f in os.listdir(ckpt) if f.endswith(\".pt\")]\n",
    "\n",
    "        model = BiLSTM(params['input_size'], params['hidden_size'], \n",
    "                    params['num_layers'], params['num_classes']).to(c.device)\n",
    "        model.load_state_dict(torch.load(f\"{ckpt}/{ckpts[0]}\"))\n",
    "        model.eval()\n",
    "        \n",
    "        explainer = LRP(model)\n",
    "        dataset_batch = DataLoader(dataset, batch_size=params['batch_size'], shuffle=False, collate_fn=model.collate)\n",
    "        for data, labels, _ in dataset_batch:\n",
    "            data = data.to(c.device)\n",
    "            relevances = explainer.get_relevance(data)\n",
    "            num_classes = relevances.shape[3]\n",
    "            \n",
    "            for i, d, l in zip(range(len(data)), data, labels):\n",
    "                fig, axs = plt.subplots(1, num_classes, figsize=(20, 5))\n",
    "                for class_idx in range(num_classes):\n",
    "                    relevance_class = relevances[i, :, :, class_idx].sum(axis=1) # sum over input channels\n",
    "                    real, imag = data[i, :, 0].cpu().numpy(), data[i, :, 1].cpu().numpy()\n",
    "                    \n",
    "                    axs[class_idx].plot(real, color='gray', label='Real')\n",
    "                    \n",
    "                    relv_pos = np.where(relevance_class > 0, relevance_class, np.nan)\n",
    "                    relv_neg = np.where(relevance_class < 0, relevance_class, np.nan)\n",
    "                    \n",
    "                    ax2 = axs[class_idx].twinx()\n",
    "                    ax2.plot(relv_pos, color='red', label='Positive Relevance')\n",
    "                    ax2.plot(relv_neg, color='blue', label='Negative Relevance')\n",
    "                    # ax2.set_ylim(-0.5, 0.5)\n",
    "                    ax2.set_yticks([])\n",
    "        \n",
    "                plt.subtitle(f'LRP values for {l} sample {i} at SNR -{snr}dB')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                break\n",
    "            break\n",
    "        break            \n",
    "        \n",
    "explain_mode(explain_dataset)\n",
    "\n",
    "# 데이터셋을 불러오는 과정에서 패딩은 해당 배치에서 가장 길이가 긴 값을 기준으로 이루어지기 때문에 data의 일부가 0으로 되어있을 수 있음\n",
    "# \n",
    "# shap 값은 해당 데이터의 실제 값과 비교하여 어떤 부분이 중요한지를 나타내는 값이기 때문에 0으로 패딩된 부분은 중요하지 않음\n",
    "# 따라서 shap 값을 보기 위해서는 해당 데이터의 실제 값만을 보는 것이 좋음\n",
    "# 이를 반영해 코드는 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_mode(dataset):\n",
    "    print(f\"Size of test dataset: {len(dataset)}\")\n",
    "    for snr in range(params['snr_min'], params['snr_max']+1, 2):\n",
    "        ckpt = os.path.join(\"./ckpts/\", snr_str:=f\"-{snr}dB\" if snr != 0 else \"0dB\")\n",
    "        ckpts = [f for f in os.listdir(ckpt) if f.endswith(\".pt\")]\n",
    "\n",
    "        LSTMmodel = BiLSTM(params['input_size'], params['hidden_size'], \n",
    "                    params['num_layers'], params['num_classes']).to(c.device)\n",
    "        LSTMmodel.load_state_dict(torch.load(f\"{ckpt}/{ckpts[0]}\"))\n",
    "        \n",
    "        \n",
    "        dataset_batch = DataLoader(dataset, batch_size=params['batch_size'], shuffle=False, collate_fn=LSTMmodel.collate)\n",
    "        for data, labels, _ in dataset_batch:\n",
    "            data = data.to(c.device)\n",
    "            \n",
    "            for i, d, l in zip(range(len(data)), data, labels):                \n",
    "                fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "                for class_idx in range(2): \n",
    "                    # shap_class = shap_values[i, :, :, class_idx].sum(axis=1)\n",
    "                    \n",
    "                    real_data = data[i, :, 0].cpu().numpy()\n",
    "                    imag_data = data[i, :, 1].cpu().numpy()\n",
    "                    \n",
    "                    axs[class_idx].plot(real_data, color='blue') # plot(x, y)\n",
    "                    axs[class_idx+1].plot(imag_data, color='orange')\n",
    "                    # shap_pos = np.where(shap_class >= 0, shap_class, np.nan)\n",
    "                    # shap_neg = np.where(shap_class < 0, shap_class, np.nan)\n",
    "                    \n",
    "                    # ax2 = axs[class_idx].twinx()\n",
    "                    # ax2.plot(shap_pos, label='Positive SHAP', color='orange', linestyle='--', alpha=0.7)\n",
    "                    # ax2.plot(shap_neg, label='Negative SHAP', color='blue', linestyle='--', alpha=0.7)\n",
    " \n",
    "                    # ax2.set_ylim(-0.06, 0.053)\n",
    "                    # ax2.set_yticks([])\n",
    "                    # ax2.legend(loc='upper right')\n",
    "                    \n",
    "                    # axs[class_idx].set_title(f'Class {class_idx}')\n",
    "                    # axs[class_idx].set_xlabel('Time')\n",
    "                    # axs[class_idx].set_ylabel('SHAP value')\n",
    "                    \n",
    "                    # axs[class_idx].grid(True)\n",
    "                    \n",
    "                plt.suptitle(f'SHAP values for {l} Sample {i} at SNR {snr_str}')\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                plt.show()\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "        \n",
    "explain_mode(explain_dataset)\n",
    "\n",
    "# 데이터셋을 불러오는 과정에서 패딩은 해당 배치에서 가장 길이가 긴 값을 기준으로 이루어지기 때문에 data의 일부가 0으로 되어있을 수 있음\n",
    "# \n",
    "# shap 값은 해당 데이터의 실제 값과 비교하여 어떤 부분이 중요한지를 나타내는 값이기 때문에 0으로 패딩된 부분은 중요하지 않음\n",
    "# 따라서 shap 값을 보기 위해서는 해당 데이터의 실제 값만을 보는 것이 좋음\n",
    "# 이를 반영해 코드는 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from TripletConvolution import TCN, trainTCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "signalTypes = ['Barker', 'Costas', 'Frank', 'LFM', 'P1', 'P2', 'P3', 'P4', 'T1', 'T2', 'T3', 'T4']\n",
    "\n",
    "RawType = \"/data/kiwan/dataset-CWD-50/\"\n",
    "TransformedTypes = {'DWT' : \"/data/kiwan/Unknown_radar_detection/Adaptive_wavelet_transform/dataset-SPWVD-denoised-Adaptive_DWT\",\n",
    "                    'CWD' : \"/data/kiwan/Unknown_radar_detection/Adaptive_wavelet_transform/240523_CWD-v1/\",\n",
    "                    'SAFI' : \"/data/kiwan/Unknown_radar_detection/Adaptive_wavelet_transform/240523_SAFI-v1/\",}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'dataset/CWD_signals.json'\n",
    "\n",
    "with open(json_file, 'r') as f:\n",
    "    SignalData = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [signal_type]>[snr_value]>[step]>[timepoint][real, imag]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class RadarSignalDataset(Dataset):\n",
    "    def __init__(self, signals_data, signal_types, snr_max=17):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_mapping = {signal: idx for idx, signal in enumerate(signalTypes)}\n",
    "\n",
    "        for signal_type in signal_types:\n",
    "            print(f\"Data loading for '{signal_type}'\", end='')\n",
    "            for snr_idx, snr in enumerate(range(0, snr_max, 2)): \n",
    "                print(\".\", end='') if snr_idx % 2 == 0 else None\n",
    "                ssnr = str(snr)\n",
    "                if ssnr in signals_data[signal_type]: \n",
    "                    signal_snr_data = signals_data[signal_type][ssnr]\n",
    "                    for signal in signal_snr_data:\n",
    "                        complex_signal = [self.convIQ(x) for x in signal]\n",
    "                        self.data.append(complex_signal)\n",
    "                        self.labels.append(signal_type)\n",
    "            print(\"Done!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def convIQ(datastring):\n",
    "        comp = complex(datastring.replace('i', 'j'))\n",
    "        return comp.real, comp.imag\n",
    "    \n",
    "    staticmethod\n",
    "    def collate(self, batch):\n",
    "        data, labels = zip(*batch)\n",
    "        padding = rnn_utils.pad_sequence([torch.tensor(seq, dtype=torch.float32) for seq in data], batch_first=True)\n",
    "        labels = [self.label_mapping[label] for label in labels]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        return padding, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeSize = 4\n",
    "dataset = RadarSignalDataset(SignalData, signalTypes[0:typeSize], snr_max=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden state와 cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))     # LSTM 출력: (배치 크기, 시퀀스 길이, hidden state 크기)\n",
    "        out = self.fc(out[:, -1, :])        # 마지막 타임스텝만 사용하여 출력 계산\n",
    "        return out\n",
    "\n",
    "input_size = 2          # (Real, Imag)\n",
    "hidden_size = 128       # LSTM hidden state size\n",
    "num_layers = 2          \n",
    "num_classes = len(signalTypes[:typeSize])  # Expected Output size\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40 \n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    rl = 0.0\n",
    "    \n",
    "    for data_batch, labels_batch in train_loader:\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch)\n",
    "\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        rl += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {rl/len(train_loader):.4f}')\n",
    "torch.save(model.state_dict(), f'./lstm_l{rl/len(train_loader):.4f}.pt')\n",
    "\n",
    "model.eval() \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_batch, labels_batch in test_loader:\n",
    "        data_batch = data_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        outputs = model(data_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_batch.size(0)\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0\n",
    "targetSNR = 0\n",
    "\n",
    "vDataset = [(d,l,s) for d, l, s, _ in Dataset if s == targetSNR and signalTypes[l] in signal_groups[f'v{version}']]\n",
    "\n",
    "plot = {}\n",
    "for data, label, _ in vDataset:\n",
    "    signalType = signalTypes[label]\n",
    "    if signalType not in plot:\n",
    "        plot[signalType] = data\n",
    "\n",
    "fig, axs = plt.subplots(1, len(plot))\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "for i, (signal_type, image) in enumerate(plot.items()):\n",
    "    axs[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axs[i].set_title(signal_type)\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn = TCN(input_channel=1).cuda() \n",
    "optimizer = optim.Adam(tcn.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "maxSNR = 2\n",
    "unique_labels = np.unique([l for _, l, _ in vDataset])\n",
    "\n",
    "for snr in range(0, maxSNR, 2):\n",
    "    snrDataset = [(d,l,s) for d, l, s in vDataset if s == snr]\n",
    "    trainTCN(tcn, optim=optimizer, dataset=snrDataset, data_type='DWT', snr=snr, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
